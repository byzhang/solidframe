#summary General overview

= Page Content =

Here's what you'll learn on this page:
  * General and structural informations about the framework.
  * How to build and test the proof of concept server.
  * How to build and test the clients for the proof of concept server
  * The design overview.
  * Some words about the ipc library.
  * Some words about socket communication design.
  * Some words about the file manage.

= General and structural informations about the framework =

Before presenting the structure of the framework, here are some general infos:
  * The build system used is cmake, and it is designed for out-of-source compilation.
  * There is no central "include" directory. Every library will be presented as a sub-folder containing the public interface header files and a "src" sub-folder containing the private sources and headers.

Here's the top level directory structure:

{{{
solidgroud/
   system/
   utility/
   algoritm/
   clientserver/
   test/
}}}

== The "system" directory ==

The *system* folder contains the *system library*, which is basically a wrapper library for system API. There are: a thread class, synchronization classes (mutex-es, conditions etc.), a thread specific wrapper class, wrappers for socket address and for files etc.

== The "utility" directory ==

This is also a one library folder containing utility classes for concepts like streams, workpools (thread pools). It also contains utility data structure classes like queue and stack (twice faster than their std equivalents). There are also some interesting goodies which you'll have to discover by yourself.

== The "algorithm" directory ==

This is a multi-library folder, containing for now only two: 
  * a protocol library (containing classes useful for text protocols - asynchronous reader (parser) and writer (response builder)); 
  * and a binary serialization library.

== The "clientserver" directory ==

It contains four libraries: core, tcp, udp and ipc. Please read the design overview paragraph.

== The "test" directory ==

This folder contains the test applications for libraries. The most important test application being the clientserver proof of concept one located at: "test/clientserver/main" (see "How to build and test the proof of concept server"). The rest of the tests are small applications written purposely for verifying certain libraries/classes.

== How is the framework thought to be used? ==

Well, I've designed the structure and the build system of the solidground framework to be used in a multiple repository manner. That means you'll have the solidground repository and your application repository. On your local copy of repositories, your application will reside withing "solidground/application" directory which must contain a "CMakeLists.txt" file which in turn will integrate your application(s) into the solidground's build system. The application folder will be quite similar to the "test" folder, except that it will not reside within the solidground repository. This design MAY impose some restrictions to the build system for your own application(s) but has the big advantage of being easy to use (and expecially easy to start using), as the cmake files will be quite similar to those from "solidground/test" directory.

= How to build and test the proof of concept server =

Before anything you must ensure you have all the needed packages installed on you Linux box. You'll need: gcc-c++, boost, boost-devel, cmake, ccache (on a fedora box: "yum install gcc-c++ boost boost-devel cmake ccache" will suffice). Eventually you'll need "subversion" to fetch a copy of solidground repository (fedora: "yum install subversion").

The solidground framework, has a commodity build script which can be used to create certain builds for certain base build systems (make or kdevelop3 - run ./build.sh with no parameters to se the options).

So lets build a (debug-)nolog version.

`solidground $ ./build.sh nolog`

`solidground $ cd build/nolog/test/clientserver/main`

`main $ make`

The above commands will star building the proof of concept clientserver test application.
When it's done the application is create within the main directory, and you can start using it:

`main $ ./test`

The application should start printing some information and offering a small CLI on which you can type help to see the commands.

The test application expects a starting port value which will be used for opening listening tcp and udp sockets (default is 1000). There are few services offered by test among which are echo and the most important, alpha.

In the following lines, to show the capabilities of the framework we'll use the alpha protocol which somehow resembles the IMAP protocol in syntax.
The alpha service will listen on (x + 114) port, where x is the value given as parameter to test application.
So, next you'll need to start a telnet on localhost 1114 (telnet 0 1114) and you should see the alpha banner.

With alpha you can list folder content:

`aa list "/home"`

Append a new file to an existing folder:

`ac store "/tmp/000002.txt" {10}`

`1234567890`

The above command will create "/tmp/000002.txt" and will write the given 10 chars on it.


Fetch files:

`ab fetch "/tmp/000002.txt"`


Now lets move forward to two more interesting commands. For that we'll need another instance of test listening on different set of ports:

`main $ ./test 2000`

We'll also need a new telnet alpha session on the new test instance:

`telnet 0 2114`

On this telnet session first copy from the banner what is between brackets (something like "2222 83886081 0") and then we'll issue the idle command:

`aa idle`

To exit from the idle command type `done`, but for now lets leave the session this way - in idle.

Now go back to the first alpha session opened on the first test instance, and issue:

`ad sendstring localhost 2222 83886081 0 "one nice string"`

(please replace "2222 83886081 0" with what you've copied from the banner of the second alpha connection)

Now go to the second alpha connection to see the marvel.

Now lets send something nicer, a file from the first alpha session:

`ae sendstream localhost 2222 83886081 0 "/tmp/000002.txt" "/tmp/rcvd-000002.txt"`

And, you guessed!, back to the second alpha connection to see a new marvel.

Notable is that the last two commands can be used with the test instances on different machines, in which case 'localhost' can be replaced with the name/address of the peer (this way the test can be started without parameter in which case in the above alpha commands you'll need to replace 2222 with 1222). Also don't forget to add new firewall rules for used ports (on my machine I'm allowing for 1114(tcp) and 1222(udp)).

In the end lets go back to the idle connection and type:

`done`
 
... to end the idle command.

Then lets remotely fetch the file stored above ("/tmp/000002.txt"): 

`bb fetch "/tmp/000002.txt" localhost 1222`

For the purpose of testing remote fetching files there is a client test application called
"alphafetch", you can build it this way:

`solidground $ cd build/nolog/test/client`
`client $ make`

and you can run it this way:

`client $ ./alphafetch localhost 2114 localhost 1222 "/tmp/000002.txt" dest_000002.txt`

or this way:

`client $ time ./alphafetch localhost 2114 localhost 1222 "/tmp/000002.txt" dest_000002.txt`

to test the speed of fetching.

*Notes:*

1) To close a connection use: 

`xx logout`

2) To close the test application type "quit" in the mini CLI

3) The remote fetching will use a local temporary file (at most 2MB) as buffer for fetching - no matter the size of the fetched file. The temp files are located in /tmp/test/. The best way to test the remote fetching is using two computers - I did it an over a wireless network I got +1MB/s transfer rate. It is not much but it is something.

= How to build and test the clients for the proof of concept server =

Some alpha commands are too difficult to be thoroughly tested by hand using telnet. So there are some client applications writen to test some of those commands. They are located in: 
 
`solidground/test/client`

Lets build em in release mode. If you haven't done so yet do: 

 `solidground$ ./build.sh release`

Then do: 
 `solidground $ cd build/release/test/client`

 `client $ make`

Now you have built three clients: *alphaclienta*, *alphafetch*, *alphastore*.

== alphafetch ==

As its name says this client tests the alpha fetch command.Usage:

 `./alphafetch alpha_addr alpha_port ipc_addr ipc_port path local_path`

Where: 

 * alpha_addr: address of the alpha server
 * alpha_port: port of the alpha server
 * ipc_addr: the base ipc address of the remote server (used for remote fetching) - use "" for local fetch
 * ipc_port: the base ipc port of the remote server (used for remote fetching) - use "" for local fetch
 * path: the path to requested file
 * local_path: the local file to write to

== alphastore ==

This one tests the alpha store command for storing a file on the server. Usage: 

 `./alphastore alpha_addr alpha_port local_path path`

Where: 

 * alpha_addr: address of the alpha server
 * alpha_port: port of the alpha server
 * local_path: the local source file
 * path: the path to destination file on the server

== alphaclienta ==
 
This is a fetch stress test for alpha server. Usage: 

 `./alphaclient thcnt addr port path tout`

Where: 
 
 * thcnt: thread and client connections count
 * addr: the address of the alpha server
 * port: the port of the alpha server
 * path: the path to the parent folder - see below
 * tout: time out between commands in msec
 

Basically it oppens thcnt connections to alpha server, and on every connection it first issues a LIST command to get the list of files located within given parent folder, then it cyclically issues FETCH commands for every file. 

While fetching alphaclienta displays certain statistical information like: 
 
 * total transfer speed in KB/s,
 * the average amount of data transfered on connections in KB,
 * the minimum amount of data in KB transfered on a connection and the count of connection with that minimum,
 * the maximum amount of data in KB transfered on a connection and the count of connection with that maximum.
 
*Example*: 
 
 `speed = 20526k/s avg = 30790k min = 29788k (1) max = 31241k (1)`
 
So we have 20MB/s total transfer rate (localhost), a current average transfer of 30 MB and one connection with a minimum transfer of 29MB and one with a maximum transfer of 31MB.

= The design overview =

The overall design of the framework is based on four concepts: pseudo-active objects, containers of objects (services), thread pools for objects, and a server which holds the services and the pools.

Here is the logical hierarchy within the framework:

  * On top and central to the architecture is the server which holds services and pools. It can be easily accessed from within any thread of it's pools (through thread specific data), and permit asynchronous signaling operations on objects.
  * The services are passive containers of objects, and have operations like: single object signaling, signal broadcasting and object visiting. Notable is also the fact that the services are also objects and they can therefor be signaled. Also the service is responsible to provide the objects with a locking mechanism (a mutex - which can be shared by multiple objects).
  * Thread pools (aka WorkPools) are somehow, active containers of objects. They actively hold objects, providing them processor time in response to signals and/or events. There are certain kinds of pools.
     # Basic Pools which execute input objects, in a queue like fashion. On this pools objects can reside until they are executed.
     # More complex pools on which objects can reside "as much as they want",  and they are provided with scheduling mechanisms (re-execution after a certain amount of time), and most important of all they can be executed when they receive signals.
     # And a very special type of pools - the communication ones (for TCP or UDP) which beside re-scheduled object execution, execution on object signaling, they also provide execution based on I/O completion events.
  * The objects are at the very base of the architecture. They must reside both on a passive container (a service) and on an active one (pool) and they usually implement state machines to complete certain tasks. Here are some examples of objects:
    # Connections for implementing high level TCP protocols like IMAP.
    # Talkers for implementing high level UDP protocols (like the one used by IPC module).
    # etc.

*Notes*:

1) A service can have lots of types of objects residing on different pools. For example the IPC service will have both Talkers (for communication over UDP) and Connections for communication over TCP.

2) Every object has a unique id (a pair of uint32s) to uniquely identify an object even after its deletion. This way if you'll want to signal some data to an object you don't neet to keep a pointer to it, and make sure it is valid until you'll be able to send the data. You keep the unique id and when the data is available you try to signal the object which can safely been deleted meanwhile.

3) There are two types of object signals: a fast, flag like one and a Command like one. The second type means sending a command to an object which on receive, it will execute the command and eventually extract needed data. For example the storage manager will asynchronously signal a stream to the object that requested it using a Command.

= Some words about the ipc library =

For now the ipc service communicates only over udp and can only send commands (i.e. objects derived from Command). Internally the ipc service uses process connectors that are objects dealing with udp problems (out-of-order buffer receiving, buffers not reaching the destination etc.). Because there is only one process connector for every peer process, and because the commands sent can be quite big (e.g. one can send file streams, like in the case of sendstream alpha command), the commands are multiplexed by the connectors so that no more than X KB are send for a command at a time if other commands are waiting.

Every this process connector has an unique id. So one can send commands to a peer process using two identifiers: either the connector id or the process' base address (the address and port on which the process is listening for udp packets - in the above examples the base addresses were (localhost 1222) and (localhost 2222) - see note 2 below).

The connector id is given to every received command. That is because usually commands are separated on two types: requests and responses. From the ipc's point of view, the difference between those two is that while requests CAN and WILL be sent to a restarted process, the responses MUST not. So when sending a request, one should use the base address while if she/he wants to send a response or more commands to the same (not-restarted) instance of a process the Connector unique id MUST be used.

Also it is the business of the command to signal back the successful arrival and completion of the command if the application logic requests that. Also the objects sending requests should implement logic for situations when responses fail to arrive.


*Notes*: 

1) In the future I may add support in ipc for signaling with some error flag the objects waiting for responses, when a peer process restart/failure is detected at ipc level.
For now I don't think it will simplify things because, as I do not implement any sort of keep alive mechanism, the restart/failure situations will only be detected if there will be activity on that connection or with that process. Also adding keep alive support is imho not so desirable from networking point of view. So considering that it is quite easy for objects to implement waiting/timeout logic I think (for now) that leaving things as they are is the best option.

2) The number of udp sockets used by ipc will increase proportionaly with the number of peer processes (i.e. a udp talker will be used for more than one process but less than M processes). The first talker added to the ipc service is considered the base talker as it is the one used for creating connection/process-connectors - think of it as a tcp listener socket which can also be used for receiving/sending data.

= Some words about socket communication design =

As far as my studies go, there are three main ways to handle communication:
  * Every connection with it own thread - blocking communication.
  * A reactor manager thread and a pool of workers handling the communication either blocking or not.
  * Handling multiple connection on a single thread using a reactor and nonblocking io or better using a asynchronous IO / proactor (io completion - in fact the io is handled by multiple kernel threads and the user space thread receives completion events).

The first model has the advantages of being easy to use - easy to write code - when the requests are simple. But think of how complex the code would get (if it's possible in a platform independent manner) if you'd want to implement something like alpha *idle* command!

The second model I dis considered for some reasons: 
  * performance related (moving a connection back and forth from manager thread to worker is quite time consuming, also the manager thread will be quite busy when handling thousands of busy connections - consuming lot of processor time).
  * code complexity related (How much should a connection keep a thread? How many worker threads should be created?)

The third model was the chosen one. The idea is that a communication thread will handle at most N connections. New threads being created when connection count increases. For every connection the communication is handled in a proactor/asynchronous manner (well on Linux the IO-completion is emulated using epoll and nonblocking IO, but the interface at connection level remains the same: it issues a send buffer/stream and/or a receive buffer/stream and waits for operation(s) completion, meanwhile the connection being able to receive other signals). 

But writing asynchronous code is not easy! The state machine of the connection can become quite complex - see the difference between the state machine of the echo connection and the one of alpha. To deal with such complexity, *SolidGround* contains the _algorithm/protocol_ library (used also by alpha protocol). It offers an easy way to design and implement complex asynchronous state machines specifically for high level protocol (like POP, IMAP etc). Please take a look at test::alpha::Connection::execute(...) (_test/clientserver/alpha/alphaconnection.cpp_) to see the implementation of the alpha connection's state machine - and think that, that state machine (or one with little improvements/additions) will suffice for most text protocols (like POP, IMAP etc).

*Notes*

1) One big advantage of the implementation that I've observed while coding - handling multiple connections on a thread allows for efficiently using thread caching (using thread specific - see _system/specific.h_). Think of how big memory improvement wold be to cache alpha commands objects on thread specific, especially for commands frequently used (as real life example think of IMAPs noop or idle commands).

= Some words about the file manager =

Lots of servers will need synchronized access to files. By synchronized I mean:

 * only one writer is allowed at a time.
 * multiple readers are allowed simultaneously.
 * no reader is allowed if there are pending writers.

Also one may limit the number of files that can be open by the server.

The SolidGround framework tries to deal with this situation, offering an asynchronous file manager. Objects like connections can request streams to a certain file and the file manager will either return the stream immediately or it will signal the stream when it will can (i.e. there are available file descriptors and no existing stream conflicts with the returned one).

More over, such servers will need temporary files and also will mostly identify a file not by its path but by an id with which the path can be computed on the fly when needed.

The file manager deals both those problems using a plugin like mechanism which allows the user to specify the key for searching a file and a mapper which will keep the open files.

For now there are only two such plugins: one for identifying files by their path, and one for temporary files (the beauty of the design comes from the fact that the support for temporary files come at the expense of some small interface changes).

*Notes:*

1) For now the files are only open in read-write mode, even if all the requested streams are for reading.

2) Also for now the mapping plugins cannot be reconfigured at runtime, but the support can be easily added.

3) The file manager also deals with the situation when files cannot be opened because the system imposed limit for open file descriptors was reached. The manager will retry to open the file later.